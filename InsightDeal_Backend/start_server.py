import os
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import re
import traceback

# âœ… í™˜ê²½ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session, joinedload
import database
import models

# âœ… Lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (ìµœì‹  ë°©ì‹)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ ì‹œì‘ ì‹œ
    print("ğŸš€ InsightDeal API ì„œë²„ ì‹œì‘")
    database.Base.metadata.create_all(bind=database.engine)
    yield
    # ì„œë²„ ì¢…ë£Œ ì‹œ
    print("ğŸ›‘ InsightDeal API ì„œë²„ ì¢…ë£Œ")

# --- FastAPI ì•± ì„¤ì • ---
app = FastAPI(lifespan=lifespan)

# âœ… CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì´ë¯¸ì§€ í´ë” ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
IMAGE_CACHE_DIR = os.path.join(SCRIPT_DIR, "image_cache")
if not os.path.exists(IMAGE_CACHE_DIR):
    os.makedirs(IMAGE_CACHE_DIR)

app.mount("/images", StaticFiles(directory=IMAGE_CACHE_DIR), name="images")

# --- ìƒí’ˆ ì •ë³´ í™•ì¥ ìœ í‹¸ í•¨ìˆ˜ë“¤ ---

def crawl_post_details(post_url: str, deal_title: str):
    """ì›ë³¸ ê²Œì‹œë¬¼ì—ì„œ ìƒì„¸ ì´ë¯¸ì§€ì™€ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    print(f"ğŸ” [Post Crawler] Starting crawl for: {post_url[:50]}...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        print(f"ğŸŒ [Post Crawler] Requesting URL with headers: {post_url}")
        response = requests.get(post_url, headers=headers, timeout=15)
        response.raise_for_status()
        print(f"âœ… [Post Crawler] Successfully fetched content. Status: {response.status_code}, Size: {len(response.content)} bytes")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ (ê¸°ì¡´ë³´ë‹¤ ë” ë§ì´)
        images = []
        print(f"ğŸ“· [Image Extractor] Starting image extraction...")
        
        # ë‹¤ì–‘í•œ img íƒœê·¸ ì„ íƒì ì‹œë„
        img_selectors = [
            'img[src*="http"]',
            'img[data-src*="http"]', 
            'img[data-original*="http"]',
            '.content img, .post-content img, .article-content img',
            '#content img, #post img, #article img'
        ]
        
        found_images = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
        
        for selector in img_selectors:
            try:
                imgs = soup.select(selector)
                print(f"ğŸ” [Image Extractor] Selector '{selector}' found {len(imgs)} images")
                
                for img in imgs:
                    img_url = img.get('src') or img.get('data-src') or img.get('data-original')
                    if img_url and img_url.startswith('http'):
                        # ì´ë¯¸ì§€ URL ì •ì œ
                        if 'gif' not in img_url.lower() and len(img_url) > 20:  # gif ë° ë„ˆë¬´ ì§§ì€ URL ì œì™¸
                            found_images.add(img_url)
                            print(f"ğŸ“· [Image Extractor] Added image: {img_url[:60]}...")
                            
            except Exception as img_error:
                print(f"âš ï¸ [Image Extractor] Error with selector '{selector}': {img_error}")
        
        # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ë¡œ ì œí•œ
        image_list = list(found_images)[:10]
        images = [{
            "url": img_url,
            "alt": f"ìƒí’ˆ ì´ë¯¸ì§€ {i+1}",
            "description": "ìƒì„¸ ì´ë¯¸ì§€"
        } for i, img_url in enumerate(image_list)]
        
        print(f"ğŸ“· [Image Extractor] Total images extracted: {len(images)}")
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        print(f"ğŸ“ [Content Extractor] Starting content extraction...")
        content_text = ""
        
        # ë‹¤ì–‘í•œ ì»¨í…ì¸  ì„ íƒì ì‹œë„
        content_selectors = [
            '.content, .post-content, .article-content',
            '#content, #post, #article',
            '.post-body, .entry-content',
            'div[class*="content"], div[class*="post"]'
        ]
        
        for selector in content_selectors:
            try:
                content_elements = soup.select(selector)
                print(f"ğŸ” [Content Extractor] Selector '{selector}' found {len(content_elements)} elements")
                
                for element in content_elements:
                    text = element.get_text(separator='\n', strip=True)
                    if len(text) > 50:  # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ
                        content_text += text + "\n\n"
                        print(f"ğŸ“ [Content Extractor] Added content block: {len(text)} characters")
                        
                if content_text:
                    break  # ì²« ë²ˆì§¸ë¡œ ì„±ê³µí•œ ì„ íƒì ì‚¬ìš©
                    
            except Exception as content_error:
                print(f"âš ï¸ [Content Extractor] Error with selector '{selector}': {content_error}")
        
        # ì»¨í…ì¸ ê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ ë°©ë²• ì‹œë„
        if not content_text.strip():
            print(f"ğŸ”„ [Content Extractor] No content found, trying fallback method...")
            # ì „ì²´ bodyì—ì„œ ì¶”ì¶œ
            body_text = soup.body.get_text(separator='\n', strip=True) if soup.body else soup.get_text(separator='\n', strip=True)
            # ìƒí’ˆê³¼ ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            lines = body_text.split('\n')
            relevant_lines = []
            for line in lines:
                if any(keyword in line for keyword in ['ìƒí’ˆ', 'ê°€ê²©', 'í• ì¸', 'ë°°ì†¥', deal_title[:10]]):
                    relevant_lines.append(line)
            content_text = '\n'.join(relevant_lines[:20])  # ìµœëŒ€ 20ì¤„
        
        # ì»¨í…ì¸  ê¸¸ì´ ì œí•œ
        if len(content_text) > 2000:
            content_text = content_text[:2000] + "..."
        
        print(f"ğŸ“ [Content Extractor] Final content length: {len(content_text)} characters")
        
        result = {
            "images": images,
            "content": content_text.strip(),
            "crawled_at": datetime.now().isoformat(),
            "source_url": post_url
        }
        
        print(f"âœ… [Post Crawler] Successfully completed crawling. Images: {len(images)}, Content: {len(content_text)} chars")
        return result
        
    except requests.RequestException as req_error:
        print(f"âŒ [Post Crawler] Request failed for {post_url}: {req_error}")
        return {
            "images": [],
            "content": "ì½˜í…ì¸ ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "error": f"Request error: {str(req_error)}",
            "crawled_at": datetime.now().isoformat()
        }
    except Exception as e:
        print(f"âŒ [Post Crawler] Unexpected error for {post_url}: {e}")
        print(f"ğŸ” [Post Crawler] Traceback: {traceback.format_exc()}")
        return {
            "images": [],
            "content": "ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "error": str(e),
            "crawled_at": datetime.now().isoformat()
        }

def analyze_product_with_ai(deal_title: str, content: str, images: list):
    """ìƒí’ˆ ì •ë³´ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì •ì œ"""
    print(f"ğŸ¤– [AI Analyzer] Starting AI analysis for: {deal_title[:30]}...")
    
    try:
        # ê°„ë‹¨í•œ AI ë¶„ì„ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” OpenAI/Claude API ì‚¬ìš©)
        # ì—¬ê¸°ì„œëŠ” ë°˜í™˜ë§Œ í•¨
        analysis_result = {
            "productSummary": deal_title,
            "keyFeatures": [
                "ê³ í’ˆì§ˆ ìƒí’ˆ",
                "í•©ë¦¬ì  ê°€ê²©",
                "ë¹ ë¥¸ ë°°ì†¡"
            ],
            "recommended": True,
            "analysisConfidence": 85.0,
            "analyzedAt": datetime.now().isoformat()
        }
        
        print(f"âœ… [AI Analyzer] Analysis completed with confidence: {analysis_result['analysisConfidence']}%")
        return analysis_result
        
    except Exception as e:
        print(f"âŒ [AI Analyzer] Analysis failed: {e}")
        return {
            "productSummary": deal_title,
            "keyFeatures": [],
            "recommended": False,
            "analysisConfidence": 0.0,
            "error": str(e),
            "analyzedAt": datetime.now().isoformat()
        }

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/")
def read_root():
    return {"message": "InsightDeal API Server is running!", "version": "2.2"}

@app.get("/api/deals")
def get_deals_list(page: int = 1, page_size: int = 20, community_id: int = None):
    """ë”œ ëª©ë¡ ì¡°íšŒ (500 ì˜¤ë¥˜ í•´ê²° ë²„ì „)"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ“Š [Deals List] Request - Page: {page}, Size: {page_size}, Community: {community_id}")
        offset = (page - 1) * page_size
        
        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„±
        query = db.query(models.Deal)
        
        # ì»¤ë®¤ë‹ˆí‹° í•„í„°ë§ (ì˜µì…˜)
        if community_id:
            query = query.filter(models.Deal.source_community_id == community_id)
            print(f"ğŸ” [Deals List] Filtering by community ID: {community_id}")
        
        # ê´€ê³„ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ ë° ì •ë ¬
        deals_from_db = (query
                        .options(joinedload(models.Deal.community))
                        .order_by(models.Deal.indexed_at.desc())
                        .offset(offset)
                        .limit(page_size)
                        .all())
        
        print(f"ğŸ“„ [Deals List] Found {len(deals_from_db)} deals from database")
        
        results = []
        for deal in deals_from_db:
            try:
                # ì»¤ë®¤ë‹ˆí‹° ì •ë³´ ì•ˆì „í•˜ê²Œ ì¡°íšŒ
                if hasattr(deal, 'community') and deal.community:
                    community_name = deal.community.name
                else:
                    # ê´€ê³„ê°€ ì—†ì„ ê²½ìš° ì§ì ‘ ì¡°íšŒ
                    community = db.query(models.Community).filter(
                        models.Community.id == deal.source_community_id
                    ).first()
                    community_name = community.name if community else "Unknown"
                
                deal_data = {
                    "id": deal.id,
                    "title": deal.title or "",
                    "community": community_name,
                    "shopName": deal.shop_name or "",
                    "price": deal.price or "",
                    "shippingFee": deal.shipping_fee or "",
                    "imageUrl": deal.image_url or "",
                    "category": deal.category or "ê¸°íƒ€",
                    "is_closed": getattr(deal, 'is_closed', False),
                    "deal_type": getattr(deal, 'deal_type', 'ì¼ë°˜'),
                    "ecommerce_link": deal.ecommerce_link or "",
                    "indexed_at": deal.indexed_at.isoformat() if deal.indexed_at else None
                }
                results.append(deal_data)
                
            except Exception as deal_error:
                print(f"âš ï¸ [Deals List] Deal {deal.id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆë›°): {deal_error}")
                continue
        
        print(f"âœ… [Deals List] Successfully returned {len(results)}ê°œ ë”œ")
        return results
        
    except Exception as e:
        print(f"âŒ [Deals List] ë”œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        print(f"ğŸ” [Deals List] Traceback: {traceback.format_exc()}")
        return []
        
    finally:
        db.close()

@app.get("/api/communities")  
def get_communities():
    """ì»¤ë®¤ë‹ˆí‹° ëª©ë¡ ì¡°íšŒ"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ  [Communities] Fetching communities list")
        communities = db.query(models.Community).all()
        result = [{
            "id": comm.id,
            "name": comm.name,
            "base_url": comm.base_url
        } for comm in communities]
        print(f"âœ… [Communities] Found {len(result)} communities")
        return result
    except Exception as e:
        print(f"âŒ [Communities] ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    finally:
        db.close()

@app.get("/api/deals/{deal_id}")
def get_deal_detail(deal_id: int):
    """íŠ¹ì • ë”œì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ” [Deal Detail] Fetching deal ID: {deal_id}")
        deal = db.query(models.Deal).filter(models.Deal.id == deal_id).first()
        if not deal:
            print(f"âŒ [Deal Detail] Deal not found: {deal_id}")
            raise HTTPException(status_code=404, detail="Deal not found")
        
        # ì»¤ë®¤ë‹ˆí‹° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        community = db.query(models.Community).filter(
            models.Community.id == deal.source_community_id
        ).first()
        
        result = {
            "id": deal.id,
            "title": deal.title,
            "community": community.name if community else "Unknown",
            "shopName": deal.shop_name,
            "price": deal.price,
            "shippingFee": deal.shipping_fee,
            "category": deal.category,
            "postLink": deal.post_link,
            "ecommerceLink": deal.ecommerce_link,
            "imageUrl": deal.image_url,
            "indexedAt": deal.indexed_at.isoformat() if deal.indexed_at else None,
            "dealType": getattr(deal, 'deal_type', 'ì¼ë°˜'),
            "isClosed": getattr(deal, 'is_closed', False),
            "contentHtml": getattr(deal, 'content_html', None)
        }
        
        print(f"âœ… [Deal Detail] Successfully fetched deal: {deal.title[:30]}...")
        return result
    except Exception as e:
        print(f"âŒ [Deal Detail] ë”œ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        print(f"ğŸ” [Deal Detail] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

# âœ… NEW: ìƒí’ˆ ì •ë³´ í™•ì¥ API 
@app.get("/api/deals/{deal_id}/enhanced-info")
def get_enhanced_deal_info(deal_id: int):
    """ë”œì˜ í–¥ìƒëœ ì •ë³´ (ìƒì„¸ ì´ë¯¸ì§€, AI ë¶„ì„)"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸš€ [Enhanced Info] Starting enhanced info fetch for deal: {deal_id}")
        
        # ê¸°ë³¸ ë”œ ì •ë³´ ì¡°íšŒ
        deal = db.query(models.Deal).filter(models.Deal.id == deal_id).first()
        if not deal:
            print(f"âŒ [Enhanced Info] Deal not found: {deal_id}")
            raise HTTPException(status_code=404, detail="Deal not found")
        
        print(f"ğŸ“„ [Enhanced Info] Found deal: {deal.title[:30]}...")
        
        # 1. ê²Œì‹œ ì‹œê°„ ì •ë³´
        posted_time_info = {
            "indexedAt": deal.indexed_at.isoformat() if deal.indexed_at else None,  # indexed_at â†’ indexedAt
            "formattedTime": None,         # formatted_time â†’ formattedTime
            "timeAgo": None               # time_ago â†’ timeAgo
        }
        
        if deal.indexed_at:
            try:
                from datetime import timezone, timedelta
                now = datetime.now(timezone.utc)
                indexed_time = deal.indexed_at.replace(tzinfo=timezone.utc) if deal.indexed_at.tzinfo is None else deal.indexed_at
                time_diff = now - indexed_time
                
                # ì‹œê°„ í¬ë§·íŒ…
                if time_diff.days > 0:
                    posted_time_info["timeAgo"] = f"{time_diff.days}ì¼ ì „"
                elif time_diff.seconds > 3600:
                    hours = time_diff.seconds // 3600
                    posted_time_info["timeAgo"] = f"{hours}ì‹œê°„ ì „"
                else:
                    minutes = time_diff.seconds // 60
                    posted_time_info["timeAgo"] = f"{minutes}ë¶„ ì „"
                    
                posted_time_info["formattedTime"] = indexed_time.strftime("%mì›” %dì¼ %H:%M")
                print(f"â° [Enhanced Info] Time info calculated: {posted_time_info['timeAgo']}")
                
            except Exception as time_error:
                print(f"âš ï¸ [Enhanced Info] Time calculation error: {time_error}")
        
        # 2. ì›ë³¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        product_detail = None
        if deal.post_link:
            print(f"ğŸ” [Enhanced Info] Starting post crawling for: {deal.post_link}")
            product_detail = crawl_post_details(deal.post_link, deal.title)
            
            # 3. AI ë¶„ì„ (ì˜µì…˜)
            if product_detail and product_detail.get('content'):
                print(f"ğŸ¤– [Enhanced Info] Starting AI analysis...")
                ai_analysis = analyze_product_with_ai(
                    deal.title, 
                    product_detail['content'], 
                    product_detail.get('images', [])
                )
                product_detail['ai_analysis'] = ai_analysis
        else:
            print(f"âš ï¸ [Enhanced Info] No post link available for crawling")
        
        # ìµœì¢… ê²°ê³¼ ì¡°í•©
        result = {
            "dealId": deal_id,                    # deal_id â†’ dealId
            "postedTime": posted_time_info,       # posted_time â†’ postedTime
            "productDetail": product_detail,      # product_detail â†’ productDetail  
            "enhancedAt": datetime.now().isoformat()  # enhanced_at â†’ enhancedAt
        }
        
        print(f"âœ… [Enhanced Info] Successfully completed enhanced info fetch")
        print(f"ğŸ“ˆ [Enhanced Info] Result summary:")
        print(f"  - Posted time: {posted_time_info.get('time_ago', 'Unknown')}")
        print(f"  - Images found: {len(product_detail.get('images', [])) if product_detail else 0}")
        print(f"  - Content length: {len(product_detail.get('content', '')) if product_detail else 0} chars")
        print(f"  - AI analysis: {'Yes' if product_detail and product_detail.get('ai_analysis') else 'No'}")
        
        return result
        
    except HTTPException:
        raise  # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        print(f"âŒ [Enhanced Info] Enhanced info ì¡°íšŒ ì‹¤íŒ¨: {e}")
        print(f"ğŸ” [Enhanced Info] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Enhanced info fetch failed: {str(e)}")
    finally:
        db.close()

@app.get("/api/deals/{deal_id}/history")
def get_price_history(deal_id: int):
    """íŠ¹ì • ë”œì˜ ê°€ê²© ë³€ë™ ê¸°ë¡ì„ ì‹œê°„ìˆœìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ“ˆ [Price History] Fetching price history for deal: {deal_id}")
        price_records = db.query(models.PriceHistory).filter(
            models.PriceHistory.deal_id == deal_id
        ).order_by(models.PriceHistory.checked_at.asc()).all()
        
        if not price_records:
            print(f"ğŸ“Š [Price History] No price history found for deal: {deal_id}")
            return []
        
        result = [{
            "price": record.price,
            "checked_at": record.checked_at.isoformat()
        } for record in price_records]
        
        print(f"âœ… [Price History] Found {len(result)} price records")
        return result
        
    except Exception as e:
        print(f"âŒ [Price History] ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []
    finally:
        db.close()

@app.get("/health")
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ InsightDeal API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤: {os.getenv('DATABASE_URL', 'Not configured')}")
    print(f"ğŸŒ API ë¬¸ì„œ: http://localhost:8000/docs")
    
    uvicorn.run(
        "start_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        reload_dirs=["."],
        log_level="info"
    )