import os
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import re
import traceback

# âœ… í™˜ê²½ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session, joinedload
import database
import models

# âœ… Lifespan ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (ìµœì‹  ë°©ì‹)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ ì‹œì‘ ì‹œ
    print("ğŸš€ InsightDeal API ì„œë²„ ì‹œì‘")
    database.Base.metadata.create_all(bind=database.engine)
    yield
    # ì„œë²„ ì¢…ë£Œ ì‹œ
    print("ğŸ›‘ InsightDeal API ì„œë²„ ì¢…ë£Œ")

# --- FastAPI ì•± ì„¤ì • ---
app = FastAPI(lifespan=lifespan)

# âœ… CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì´ë¯¸ì§€ í´ë” ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
IMAGE_CACHE_DIR = os.path.join(SCRIPT_DIR, "image_cache")
if not os.path.exists(IMAGE_CACHE_DIR):
    os.makedirs(IMAGE_CACHE_DIR)

app.mount("/images", StaticFiles(directory=IMAGE_CACHE_DIR), name="images")

# --- âœ¨ ìƒˆë¡œìš´ í†µí•© ìŠ¤í¬ë˜í¼ ë¼ìš°íŒ… ì‹œìŠ¤í…œ ---
def smart_crawl_post_details(post_url: str, deal_title: str):
    """ë„ë©”ì¸ë³„ë¡œ ìµœì í™”ëœ ìŠ¤í¬ë˜í¼ë¥¼ ìë™ ë¼ìš°íŒ…í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    print(f"ğŸ¯ [Smart Crawler] Starting smart crawl for: {post_url[:50]}...")
    
    try:
        # DB ì„¸ì…˜ ìƒì„±
        db_session = database.SessionLocal()
        
        # 1. ë„ë©”ì¸ ê°ì§€ ë° ì ì ˆí•œ ìŠ¤í¬ë˜í¼ ì„ íƒ
        if 'ppomppu.co.kr' in post_url:
            if 'ppomppu4' in post_url:  # ì•Œë¦¬ë½ë¿Œ
                print("ğŸ” [Smart Crawler] Routing to: ì•Œë¦¬ë½ë¿Œ ìŠ¤í¬ë˜í¼")
                from scrapers.alippomppu_scraper import AlippomppuScraper
                with AlippomppuScraper(db_session) as scraper:
                    return scraper.get_post_details(post_url)
            elif 'ppomppu3' in post_url:  # í•´ì™¸ë½ë¿Œ
                print("ğŸ” [Smart Crawler] Routing to: í•´ì™¸ë½ë¿Œ ìŠ¤í¬ë˜í¼")
                from scrapers.ppomppu_overseas_scraper import PpomppuOverseasScraper
                with PpomppuOverseasScraper(db_session) as scraper:
                    return scraper.get_post_details(post_url)
            else:  # ì¼ë°˜ë½ë¿Œ
                print("ğŸ” [Smart Crawler] Routing to: ë½ë¿Œ ìŠ¤í¬ë˜í¼")
                from scrapers.ppomppu_scraper import PpomppuScraper
                with PpomppuScraper(db_session) as scraper:
                    return scraper.get_post_details(post_url)
                    
        elif 'ruliweb.com' in post_url:
            print("ğŸ” [Smart Crawler] Routing to: ë£¨ë¦¬ì›¹ ìŠ¤í¬ë˜í¼")
            from scrapers.ruliweb_scraper import RuliwebScraper
            with RuliwebScraper(db_session) as scraper:
                return scraper.get_post_details(post_url)
                
        elif 'clien.net' in post_url:
            print("ğŸ” [Smart Crawler] Routing to: í´ë¦¬ì•™ ìŠ¤í¬ë˜í¼")
            from scrapers.clien_scraper import ClienScraper
            with ClienScraper(db_session) as scraper:
                return scraper.get_post_details(post_url)
                
        elif 'quasarzone.com' in post_url:
            print("ğŸ” [Smart Crawler] Routing to: í€˜ì´ì‚¬ì¡´ ìŠ¤í¬ë˜í¼")
            from scrapers.quasarzone_scraper import QuasarzoneScraper
            with QuasarzoneScraper(db_session) as scraper:
                return scraper.get_post_details(post_url)
                
        elif 'fmkorea.com' in post_url:
            print("ğŸ” [Smart Crawler] Routing to: í¨ì½” ìŠ¤í¬ë˜í¼")
            from scrapers.fmkorea_scraper import FmkoreaScraper
            with FmkoreaScraper(db_session) as scraper:
                return scraper.get_post_details(post_url)
                
        elif 'bbasak.com' in post_url:
            print("ğŸ” [Smart Crawler] Routing to: ë¹ ì‚­ ìŠ¤í¬ë˜í¼")
            from scrapers.bbasak_base_scraper import BbasakBaseScraper
            with BbasakBaseScraper(db_session, "ë¹ ì‚­", post_url) as scraper:
                return scraper.get_post_details(post_url)
                
        else:
            print("âš ï¸ [Smart Crawler] Unknown site, falling back to generic crawler")
            return fallback_crawl_post_details(post_url, deal_title)
            
    except Exception as e:
        print(f"âŒ [Smart Crawler] Smart crawling failed: {e}")
        print(f"ğŸ” [Smart Crawler] Traceback: {traceback.format_exc()}")
        # ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
        return fallback_crawl_post_details(post_url, deal_title)
    finally:
        if 'db_session' in locals():
            db_session.close()

def fallback_crawl_post_details(post_url: str, deal_title: str):
    """ê¸°ì¡´ ë°©ì‹ì˜ ë²”ìš© í¬ë¡¤ëŸ¬ (ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš©)"""
    print(f"ğŸ”„ [Fallback Crawler] Using fallback method for: {post_url[:50]}...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        }
        
        response = requests.get(post_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë²”ìš© ì´ë¯¸ì§€ ì¶”ì¶œ
        images = []
        for img in soup.find_all('img'):
            img_src = img.get('src')
            if img_src and 'http' in img_src and len(img_src) > 20:
                if not any(keyword in img_src.lower() for keyword in ['icon', 'logo', 'emoticon']):
                    images.append({
                        "url": img_src,
                        "alt": f"ì´ë¯¸ì§€ {len(images)+1}",
                        "description": "ê²Œì‹œê¸€ ì´ë¯¸ì§€"
                    })
                    if len(images) >= 5:  # ìµœëŒ€ 5ê°œ
                        break
        
        # ë²”ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content_text = soup.get_text(separator=' ', strip=True)
        if len(content_text) > 500:
            content_text = content_text[:500] + "..."
        
        result = {
            "images": images,
            "content": content_text,
            "posted_time": None,
            "crawled_at": datetime.now().isoformat(),
            "source_url": post_url,
            "method": "fallback"
        }
        
        print(f"âœ… [Fallback Crawler] Success! Images: {len(images)}, Content: {len(content_text)} chars")
        return result
        
    except Exception as e:
        print(f"âŒ [Fallback Crawler] Fallback also failed: {e}")
        return {
            "images": [],
            "content": "ê²Œì‹œê¸€ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "posted_time": None,
            "error": str(e),
            "crawled_at": datetime.now().isoformat()
        }

def analyze_product_with_ai(deal_title: str, content: str, images: list):
    """ìƒí’ˆ ì •ë³´ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ì •ì œ"""
    print(f"ğŸ¤– [AI Analyzer] Starting AI analysis for: {deal_title[:30]}...")
    
    try:
        analysis_result = {
            "productSummary": deal_title,
            "keyFeatures": ["ê³ í’ˆì§ˆ ìƒí’ˆ", "í•©ë¦¬ì  ê°€ê²©", "ë¹ ë¥¸ ë°°ì†¡"],
            "recommended": True,
            "analysisConfidence": 85.0,
            "analyzedAt": datetime.now().isoformat()
        }
        
        print(f"âœ… [AI Analyzer] Analysis completed with confidence: {analysis_result['analysisConfidence']}%")
        return analysis_result
        
    except Exception as e:
        print(f"âŒ [AI Analyzer] Analysis failed: {e}")
        return {
            "productSummary": deal_title,
            "keyFeatures": [],
            "recommended": False,
            "analysisConfidence": 0.0,
            "error": str(e),
            "analyzedAt": datetime.now().isoformat()
        }

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/")
def read_root():
    return {"message": "InsightDeal API Server is running!", "version": "3.0 - Smart Routing"}

@app.get("/api/deals")
def get_deals_list(page: int = 1, page_size: int = 20, community_id: int = None):
    """ë”œ ëª©ë¡ ì¡°íšŒ (500 ì˜¤ë¥˜ í•´ê²° ë²„ì „)"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ“Š [Deals List] Request - Page: {page}, Size: {page_size}, Community: {community_id}")
        offset = (page - 1) * page_size
        
        query = db.query(models.Deal)
        
        if community_id:
            query = query.filter(models.Deal.source_community_id == community_id)
            print(f"ğŸ” [Deals List] Filtering by community ID: {community_id}")
        
        deals_from_db = (query
                        .options(joinedload(models.Deal.community))
                        .order_by(models.Deal.indexed_at.desc())
                        .offset(offset)
                        .limit(page_size)
                        .all())
        
        print(f"ğŸ“„ [Deals List] Found {len(deals_from_db)} deals from database")
        
        results = []
        for deal in deals_from_db:
            try:
                if hasattr(deal, 'community') and deal.community:
                    community_name = deal.community.name
                else:
                    community = db.query(models.Community).filter(
                        models.Community.id == deal.source_community_id
                    ).first()
                    community_name = community.name if community else "Unknown"
                
                deal_data = {
                    "id": deal.id,
                    "title": deal.title or "",
                    "community": community_name,
                    "shopName": deal.shop_name or "",
                    "price": deal.price or "",
                    "shippingFee": deal.shipping_fee or "",
                    "imageUrl": deal.image_url or "",
                    "category": deal.category or "ê¸°íƒ€",
                    "is_closed": getattr(deal, 'is_closed', False),
                    "deal_type": getattr(deal, 'deal_type', 'ì¼ë°˜'),
                    "ecommerce_link": deal.ecommerce_link or "",
                    "indexed_at": deal.indexed_at.isoformat() if deal.indexed_at else None
                }
                results.append(deal_data)
                
            except Exception as deal_error:
                print(f"âš ï¸ [Deals List] Deal {deal.id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆë›°): {deal_error}")
                continue
        
        print(f"âœ… [Deals List] Successfully returned {len(results)}ê°œ ë”œ")
        return results
        
    except Exception as e:
        print(f"âŒ [Deals List] ë”œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        print(f"ğŸ” [Deals List] Traceback: {traceback.format_exc()}")
        return []
        
    finally:
        db.close()

@app.get("/api/communities")  
def get_communities():
    """ì»¤ë®¤ë‹ˆí‹° ëª©ë¡ ì¡°íšŒ"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ  [Communities] Fetching communities list")
        communities = db.query(models.Community).all()
        result = [{
            "id": comm.id,
            "name": comm.name,
            "base_url": comm.base_url
        } for comm in communities]
        print(f"âœ… [Communities] Found {len(result)} communities")
        return result
    except Exception as e:
        print(f"âŒ [Communities] ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    finally:
        db.close()

@app.get("/api/deals/{deal_id}")
def get_deal_detail(deal_id: int):
    """íŠ¹ì • ë”œì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ” [Deal Detail] Fetching deal ID: {deal_id}")
        deal = db.query(models.Deal).filter(models.Deal.id == deal_id).first()
        if not deal:
            print(f"âŒ [Deal Detail] Deal not found: {deal_id}")
            raise HTTPException(status_code=404, detail="Deal not found")
        
        community = db.query(models.Community).filter(
            models.Community.id == deal.source_community_id
        ).first()
        
        result = {
            "id": deal.id,
            "title": deal.title,
            "community": community.name if community else "Unknown",
            "shopName": deal.shop_name,
            "price": deal.price,
            "shippingFee": deal.shipping_fee,
            "category": deal.category,
            "postLink": deal.post_link,
            "ecommerceLink": deal.ecommerce_link,
            "imageUrl": deal.image_url,
            "indexedAt": deal.indexed_at.isoformat() if deal.indexed_at else None,
            "dealType": getattr(deal, 'deal_type', 'ì¼ë°˜'),
            "isClosed": getattr(deal, 'is_closed', False),
            "contentHtml": getattr(deal, 'content_html', None)
        }
        
        print(f"âœ… [Deal Detail] Successfully fetched deal: {deal.title[:30]}...")
        return result
    except Exception as e:
        print(f"âŒ [Deal Detail] ë”œ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        print(f"ğŸ” [Deal Detail] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        db.close()

# âœ… NEW: ìƒí’ˆ ì •ë³´ í™•ì¥ API (ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… ì ìš©)
@app.get("/api/deals/{deal_id}/enhanced-info")
def get_enhanced_deal_info(deal_id: int):
    """ë”œì˜ í–¥ìƒëœ ì •ë³´ (ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ìŠ¤í¬ë˜í¼ ì‚¬ìš©)"""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸš€ [Enhanced Info] Starting enhanced info fetch for deal: {deal_id}")
        
        deal = db.query(models.Deal).filter(models.Deal.id == deal_id).first()
        if not deal:
            print(f"âŒ [Enhanced Info] Deal not found: {deal_id}")
            raise HTTPException(status_code=404, detail="Deal not found")
        
        print(f"ğŸ“„ [Enhanced Info] Found deal: {deal.title[:30]}...")
        
        # 1. ê²Œì‹œ ì‹œê°„ ì •ë³´
        posted_time_info = {
            "indexedAt": deal.indexed_at.isoformat() if deal.indexed_at else None,
            "formattedTime": None,
            "timeAgo": None
        }
        
        if deal.indexed_at:
            try:
                from datetime import timezone
                now = datetime.now(timezone.utc)
                indexed_time = deal.indexed_at.replace(tzinfo=timezone.utc) if deal.indexed_at.tzinfo is None else deal.indexed_at
                time_diff = now - indexed_time
                
                if time_diff.days > 0:
                    posted_time_info["timeAgo"] = f"{time_diff.days}ì¼ ì „"
                elif time_diff.seconds > 3600:
                    hours = time_diff.seconds // 3600
                    posted_time_info["timeAgo"] = f"{hours}ì‹œê°„ ì „"
                else:
                    minutes = time_diff.seconds // 60
                    posted_time_info["timeAgo"] = f"{minutes}ë¶„ ì „"
                    
                posted_time_info["formattedTime"] = indexed_time.strftime("%mì›” %dì¼ %H:%M")
                print(f"â° [Enhanced Info] Time info calculated: {posted_time_info['timeAgo']}")
                
            except Exception as time_error:
                print(f"âš ï¸ [Enhanced Info] Time calculation error: {time_error}")
        
        # 2. âœ¨ ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…ìœ¼ë¡œ ì›ë³¸ ê²Œì‹œë¬¼ í¬ë¡¤ë§
        product_detail = None
        if deal.post_link:
            print(f"ğŸ¯ [Enhanced Info] Using smart routing for: {deal.post_link}")
            product_detail = smart_crawl_post_details(deal.post_link, deal.title)
            
            if product_detail and product_detail.get('content'):
                print(f"ğŸ¤– [Enhanced Info] Starting AI analysis...")
                ai_analysis = analyze_product_with_ai(
                    deal.title, 
                    product_detail['content'], 
                    product_detail.get('images', [])
                )
                product_detail['ai_analysis'] = ai_analysis
        else:
            print(f"âš ï¸ [Enhanced Info] No post link available for crawling")
        
        result = {
            "dealId": deal_id,
            "postedTime": posted_time_info,
            "productDetail": product_detail,
            "enhancedAt": datetime.now().isoformat()
        }
        
        print(f"âœ… [Enhanced Info] Successfully completed enhanced info fetch with smart routing")
        print(f"ğŸ“ˆ [Enhanced Info] Result summary:")
        print(f"  - Posted time: {posted_time_info.get('timeAgo', 'Unknown')}")
        print(f"  - Images found: {len(product_detail.get('images', [])) if product_detail else 0}")
        print(f"  - Content length: {len(product_detail.get('content', '')) if product_detail else 0} chars")
        print(f"  - Site detected: {product_detail.get('detected_site', 'N/A') if product_detail else 'N/A'}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ [Enhanced Info] Enhanced info ì¡°íšŒ ì‹¤íŒ¨: {e}")
        print(f"ğŸ” [Enhanced Info] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Enhanced info fetch failed: {str(e)}")
    finally:
        db.close()

@app.get("/api/deals/{deal_id}/history")
def get_price_history(deal_id: int):
    """íŠ¹ì • ë”œì˜ ê°€ê²© ë³€ë™ ê¸°ë¡ì„ ì‹œê°„ìˆœìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db: Session = database.SessionLocal()
    try:
        print(f"ğŸ“ˆ [Price History] Fetching price history for deal: {deal_id}")
        price_records = db.query(models.PriceHistory).filter(
            models.PriceHistory.deal_id == deal_id
        ).order_by(models.PriceHistory.checked_at.asc()).all()
        
        if not price_records:
            print(f"ğŸ“Š [Price History] No price history found for deal: {deal_id}")
            return []
        
        result = [{
            "price": record.price,
            "checked_at": record.checked_at.isoformat()
        } for record in price_records]
        
        print(f"âœ… [Price History] Found {len(result)} price records")
        return result
        
    except Exception as e:
        print(f"âŒ [Price History] ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []
    finally:
        db.close()

@app.get("/health")
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ InsightDeal API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤: {os.getenv('DATABASE_URL', 'Not configured')}")
    print(f"ğŸŒ API ë¬¸ì„œ: http://localhost:8000/docs")
    print(f"ğŸ¯ ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… ì‹œìŠ¤í…œ í™œì„±í™” - 6ê°œ ì‚¬ì´íŠ¸ ì§€ì›")
    
    uvicorn.run(
        "start_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        reload_dirs=["."],
        log_level="info"
    )