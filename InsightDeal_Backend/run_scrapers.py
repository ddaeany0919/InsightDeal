import asyncio
import concurrent.futures
import time
from contextlib import contextmanager
from datetime import datetime
from typing import List, Type

from database import db_manager
from logger import logger
from config import config

# ëª¨ë“  ìŠ¤í¬ë˜í¼ import
from scrapers.alippomppu_scraper import AlippomppuScraper
from scrapers.ruliweb_scraper import RuliwebScraper
from scrapers.ppomppu_scraper import PpomppuScraper
from scrapers.ppomppu_overseas_scraper import PpomppuOverseasScraper
from scrapers.bbasak_domestic_scraper import BbasakDomesticScraper
from scrapers.bbasak_overseas_scraper import BbasakOverseasScraper
from scrapers.clien_scraper import ClienScraper
from scrapers.quasarzone_scraper import QuasarzoneScraper
from scrapers.fmkorea_scraper import FmkoreaScraper
# ... ë‹¤ë¥¸ ìŠ¤í¬ë˜í¼ë“¤

class ScraperManager:
    """ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    SCRAPERS = [
        AlippomppuScraper,
        RuliwebScraper,
        PpomppuScraper,
        PpomppuOverseasScraper,
        BbasakDomesticScraper,
        BbasakOverseasScraper,
        ClienScraper,
        QuasarzoneScraper,
        FmkoreaScraper,
    ]
    
    def __init__(self):
        self.results = {}
        self.start_time = None
    
    @contextmanager
    def execution_tracker(self):
        """ì‹¤í–‰ ì¶”ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        self.start_time = datetime.now()
        logger.info(f"ğŸš€ ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì‹œì‘: {self.start_time}")
        
        try:
            yield
        finally:
            elapsed = datetime.now() - self.start_time
            success_count = sum(1 for result in self.results.values() if result)
            total_count = len(self.results)
            
            logger.info(
                f"âœ… ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì™„ë£Œ - "
                f"ì„±ê³µ: {success_count}/{total_count}, "
                f"ì†Œìš”ì‹œê°„: {elapsed}"
            )
    
    def run_scraper(self, scraper_class: Type) -> bool:
        """ê°œë³„ ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
        scraper_name = scraper_class.__name__
        
        try:
            with db_manager.get_session() as db_session:
                with scraper_class(db_session) as scraper:
                    success = scraper.run(limit=5)
                    self.results[scraper_name] = success
                    return success
                    
        except Exception as e:
            logger.error(f"[ERROR] {scraper_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.results[scraper_name] = False
            return False
    
    def run_parallel(self, max_workers: int = 3) -> dict:
        """ë³‘ë ¬ ì‹¤í–‰ (ì œí•œëœ ë™ì‹œ ì‹¤í–‰)"""
        with self.execution_tracker():
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_scraper = {
                    executor.submit(self.run_scraper, scraper_class): scraper_class.__name__
                    for scraper_class in self.SCRAPERS
                }
                
                for future in concurrent.futures.as_completed(future_to_scraper):
                    scraper_name = future_to_scraper[future]
                    try:
                        success = future.result()
                        status = "[SUCCESS]" if success else "[FAILED]"
                        logger.info(f"{status} {scraper_name} ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"[ERROR] {scraper_name} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}")
        
        return self.results
    
    def run_sequential(self) -> dict:
        """ìˆœì°¨ ì‹¤í–‰"""
        with self.execution_tracker():
            for scraper_class in self.SCRAPERS:
                self.run_scraper(scraper_class)
                
                # ê° ìŠ¤í¬ë˜í¼ ê°„ ë”œë ˆì´
                time.sleep(config.SCRAPER_DELAY)
        
        return self.results

if __name__ == "__main__":
    manager = ScraperManager()
    
    # í™˜ê²½ì— ë”°ë¼ ì‹¤í–‰ ë°©ì‹ ì„ íƒ
    if hasattr(config, 'PARALLEL_EXECUTION') and config.PARALLEL_EXECUTION:
        results = manager.run_parallel(max_workers=3)
    else:
        results = manager.run_sequential()
    
    # ê²°ê³¼ ë³´ê³ 
    failed_scrapers = [name for name, success in results.items() if not success]
    if failed_scrapers:
        logger.warning(f"ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼: {failed_scrapers}")
