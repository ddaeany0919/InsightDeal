import os
import re
import sys
import hashlib
import requests
import logging
import time
import json
import ai_parser
import base64
from abc import ABC, abstractmethod
from urllib.parse import urlparse, urljoin, unquote, parse_qs

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium_stealth import stealth
from sqlalchemy.orm import Session
import easyocr
from pathlib import Path
import random
# --- ğŸ‘‡ [ì¶”ê°€] WebDriverWaitì„ ìœ„í•œ import ---
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
# --- [ì¶”ê°€ ì™„ë£Œ] ---

import models
import database

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
SELENIUM_TIMEOUT = int(os.getenv("SELENIUM_TIMEOUT", "5"))
SCRAPER_DELAY = int(os.getenv("SCRAPER_DELAY", "0"))
MAX_RETRY_COUNT = int(os.getenv("MAX_RETRY_COUNT", "2"))
HEADLESS = os.getenv("HEADLESS", "true").lower() == "true"
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# User-Agent ë¡œí…Œì´ì…˜
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

# --- ë¡œê±° ì„¤ì • ---
def setup_logger(name: str = "BaseScraper"):
    """UTF-8 ì¸ì½”ë”©ì„ ê°•ì œí•˜ëŠ” ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger(name)
    
    if logger.handlers:
        return logger
    
    logger.setLevel(getattr(logging, LOG_LEVEL))
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent.parent / "logs"
    log_dir.mkdir(exist_ok=True)
    
    # í¬ë§·í„°
    formatter = logging.Formatter(
        fmt='%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (UTF-8 ì¸ì½”ë”© ê°•ì œ)
    from logging.handlers import RotatingFileHandler
    file_handler = RotatingFileHandler(
        log_dir / f"scraper_{time.strftime('%Y%m%d')}.log",
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'  # âœ… UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì •
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.DEBUG)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ (UTF-8 ì„¤ì •)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logger()

# --- ì „ì—­ ì„¤ì • ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
IMAGE_CACHE_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), "image_cache")
if not os.path.exists(IMAGE_CACHE_DIR):
    os.makedirs(IMAGE_CACHE_DIR)

class BaseScraper(ABC):
    """
    ëª¨ë“  ìŠ¤í¬ë˜í¼ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ì¶”ìƒ í´ë˜ìŠ¤.
    """
    ocr_reader = None

    def __init__(self, db_session: Session, community_name: str, community_url: str):
        self.db = db_session
        self.community_name = community_name
        self.community_url = community_url
        self.base_url = f"{urlparse(community_url).scheme}://{urlparse(community_url).netloc}"
        self.driver = None
        self.wait = None  # WebDriverWait ê°ì²´ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì¶”ê°€
        self.cookies = {}
        self.limit = None
        self.retry_count = 0
        self.start_time = None
        self.scraped_count = 0
        self.community_entry = (
            self.db.query(models.Community)
            .filter(models.Community.name == self.community_name)
            .first()
        )
        if not self.community_entry:
            raise ValueError(f"'{self.community_name}' ì»¤ë®¤ë‹ˆí‹°ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if BaseScraper.ocr_reader is None:
            logger.info("Initializing EasyOCR model... (This may take a moment on first run)")
            BaseScraper.ocr_reader = easyocr.Reader(['ko', 'en'])
            logger.info("EasyOCR model loaded successfully.")
    def run_with_retry(self, limit=None) -> bool:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì‹¤í–‰"""
        for attempt in range(MAX_RETRY_COUNT):
            try:
                self.start_time = time.time()
                self.scraped_count = 0
                
                result = self.run(limit)
                
                # ì„±ëŠ¥ ë¡œê·¸
                elapsed = time.time() - self.start_time
                rate = self.scraped_count / elapsed if elapsed > 0 else 0
                
                logger.info(
                    f"[{self.community_name}] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ - "
                    f"ì²˜ë¦¬: {self.scraped_count}ê±´, "
                    f"ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ, "
                    f"ì†ë„: {rate:.2f}ê±´/ì´ˆ"
                )
                
                return True
                
            except Exception as e:
                self.retry_count = attempt + 1
                logger.warning(
                    f"[{self.community_name}] ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRY_COUNT}): {e}"
                )
                
                if attempt < MAX_RETRY_COUNT - 1:
                    delay = (2 ** attempt) + random.uniform(0, 1)  # ì§€ìˆ˜ ë°±ì˜¤í”„ + ëœë¤
                    logger.info(f"[{self.community_name}] {delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    continue
                else:
                    logger.error(f"[{self.community_name}] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return False
        
        return False

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            try:
                self.driver.quit()
                logger.debug(f"[{self.community_name}] ë“œë¼ì´ë²„ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"[{self.community_name}] ë“œë¼ì´ë²„ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if self.db:
            try:
                self.db.close()
                logger.debug(f"[{self.community_name}] DB ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"[{self.community_name}] DB ì„¸ì…˜ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    @staticmethod
    def _parse_ecommerce_link(raw_link: str) -> str:
        if not raw_link:
            return ""
        try:
            parsed_url = urlparse(raw_link)
            if 'target' in parsed_url.query:
                query_params = parse_qs(parsed_url.query)
                encoded_target = query_params.get("target", [None])[0]
                if encoded_target:
                    encoded_target += '=' * (-len(encoded_target) % 4)
                    decoded_bytes = base64.b64decode(encoded_target)
                    return decoded_bytes.decode('utf-8')
            return unquote(raw_link)
        except Exception as e:
            logger.warning(f"Failed to parse ecommerce link '{raw_link}': {e}")
            return raw_link

    @abstractmethod
    def scrape(self):
        pass

    def run(self, limit=None):
        self.limit = limit
        self.scraped_count = 0
        
        logger.info(f"[{self.community_name}] ìŠ¤í¬ë˜í•‘ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        try:
            self._create_selenium_driver()
            deals_data = self.scrape()
            
            if not deals_data:
                logger.info(f"[{self.community_name}] ìƒˆë¡œìš´ ë”œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                return False
            
            deals_data.reverse()
            new_deals_count = 0
            
            # âœ… ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
            batch_deals = []
            batch_histories = []
            
            for item in deals_data:
                self.scraped_count += 1
                
                deal_obj = item['deal']
                exists = self.db.query(models.Deal.id).filter(  # âœ… ìˆ˜ì •
                    models.Deal.post_link == deal_obj.post_link
                ).first()

                if exists:
                    continue
                
                # ê¸°ì¡´ OCR ë¡œì§
                if deal_obj.shipping_fee == 'ì •ë³´ ì—†ìŒ' and deal_obj.deal_type == 'ì¼ë°˜':
                    original_image_url = item.get('original_image_url')
                    if original_image_url:
                        _, _, ocr_text = self._download_and_get_local_path(
                            original_image_url, -1, perform_ocr=True
                        )
                        
                        if any(term in ocr_text for term in ("ë¬´ë£Œë°°ì†¡", "ë¬´ë£Œ ë°°ì†¡")):
                            logger.info(f"[OCR] '{deal_obj.title}'ì—ì„œ 'ë¬´ë£Œë°°ì†¡' ë°œê²¬. ë°°ì†¡ë¹„ ì—…ë°ì´íŠ¸")
                            deal_obj.shipping_fee = "ë¬´ë£Œ"
                
                batch_deals.append(deal_obj)
                new_deals_count += 1
            
            # âœ… ë°°ì¹˜ ì €ì¥
            if batch_deals:
                self.db.add_all(batch_deals)    # âœ… ìˆ˜ì •
                self.db.flush()                 # âœ… ìˆ˜ì •
                
                for deal in batch_deals:
                    batch_histories.append(models.PriceHistory(deal_id=deal.id, price=deal.price))
                
                self.db.add_all(batch_histories)  # âœ… ìˆ˜ì •
                self.db.commit()                  # âœ… ìˆ˜ì •
                
                logger.info(f"[{self.community_name}] {new_deals_count}ê°œì˜ ìƒˆë¡œìš´ ë”œì„ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
            else:
                logger.info(f"[{self.community_name}] ì €ì¥í•  ìƒˆë¡œìš´ ë”œì´ ì—†ìŠµë‹ˆë‹¤")
            
            return new_deals_count > 0
        
        except Exception as e:
            logger.error(f"[{self.community_name}] ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            self.db.rollback()  # âœ… ìˆ˜ì •
            return False
        finally:
            pass

    def _create_selenium_driver(self):
        """ìµœì í™”ëœ Chrome ë“œë¼ì´ë²„ ìƒì„±"""
        chrome_options = Options()
        
        # âœ… í•µì‹¬ ì„±ëŠ¥ ìµœì í™” (í’ˆì§ˆ ìœ ì§€)
        chrome_options.page_load_strategy = 'eager'  # DOM ë¡œë“œë˜ë©´ ì¦‰ì‹œ ì§„í–‰
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-software-rasterizer")
        chrome_options.add_argument("--disable-logging")
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--silent")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        
        # âœ… ë¶ˆí•„ìš”í•œ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--no-first-run")
        chrome_options.add_argument("--disable-default-apps")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        chrome_options.add_argument("--disable-client-side-phishing-detection")
        chrome_options.add_argument("--disable-sync")
        chrome_options.add_argument("--metrics-recording-only")
        chrome_options.add_argument("--no-default-browser-check")
        
        # âœ… ë©”ëª¨ë¦¬ ìµœì í™”
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=4096")
        
        # âœ… ë„¤íŠ¸ì›Œí¬ ìµœì í™”  
        chrome_options.add_argument("--aggressive-cache-discard")
        chrome_options.add_argument("--disable-background-networking")
        
        # ìœˆë„ìš° í¬ê¸° ì„¤ì •
        chrome_options.add_argument("--window-size=1920,1080")
        
        # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        if HEADLESS:
            chrome_options.add_argument("--headless")
        
        # ëœë¤ User-Agent
        user_agent = random.choice(USER_AGENTS)
        chrome_options.add_argument(f'user-agent={user_agent}')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(3)
            self.driver.implicitly_wait(5)
            self.wait = WebDriverWait(self.driver, 3)
            
            # Stealth ì„¤ì •
            stealth(
                self.driver,
                languages=["ko-KR", "ko"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True
            )
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(self.community_url)
            self.cookies = {c['name']: c['value'] for c in self.driver.get_cookies()}
            
            logger.info(f"[{self.community_name}] ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"[{self.community_name}] ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _download_and_get_local_path(self, image_url, deal_id: int, perform_ocr: bool = True):
        """ìµœì í™”ëœ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ìºì‹±"""
        placeholder = 'https://placehold.co/400x400/E9E2FD/333?text=Deal'
        
        if not isinstance(image_url, str) or not image_url or 'placehold.co' in image_url:
            return placeholder, None, ""

        try:
            url_hash = hashlib.md5(image_url.encode()).hexdigest()
            match = re.search(r'\.(jpg|jpeg|png|gif|webp)', urlparse(image_url).path, re.IGNORECASE)
            ext = match.group(0) if match else '.jpg'
            filename = f"{url_hash}{ext}"
            save_path = os.path.join(IMAGE_CACHE_DIR, filename)
            
            # ìºì‹œ í™•ì¸
            if os.path.exists(save_path):
                file_age = time.time() - os.path.getmtime(save_path)
                if file_age < 7 * 24 * 3600:  # 7ì¼ ì´ë‚´ë©´ ìºì‹œ ì‚¬ìš©
                    logger.debug(f"[ImageCache] Using cached image: {filename}")
                    ocr_text = self._ocr_image(save_path) if perform_ocr else ""
                    return f"/images/{filename}", save_path, ocr_text

            # ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
            headers = {
                'Referer': f"{urlparse(image_url).scheme}://{urlparse(image_url).netloc}/",
                'User-Agent': self.driver.execute_script("return navigator.userAgent;") if self.driver else random.choice(USER_AGENTS)
            }
            
            resp = requests.get(
                image_url, 
                headers=headers, 
                cookies=self.cookies, 
                stream=True, 
                timeout=15
            )
            resp.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logger.debug(f"[ImageCache] Downloaded new image: {filename}")
            ocr_text = self._ocr_image(save_path) if perform_ocr else ""
            
            return f"/images/{filename}", save_path, ocr_text

        except Exception as e:
            logger.warning(f"[ImageProcessing] Failed for URL: {image_url}, Error: {e}")
            return placeholder, None, ""

    @staticmethod
    def _clean_price(price_input):
        price_str = str(price_input).strip()
        if not price_str or "ì •ë³´" in price_str or "ì—†ìŒ" in price_str:
            return "ì •ë³´ ì—†ìŒ"

        is_dollar = ("$" in price_str or "ë‹¬ëŸ¬" in price_str or "dollar" in price_str.lower()) and "ì›" not in price_str
        
        if is_dollar:
            try:
                num_match = re.search(r'[\d,]+\.?\d*', price_str)
                if num_match:
                    num_str = num_match.group().replace(',', '')
                    dollar_amount = float(num_str)
                    return f"${dollar_amount:.2f}"
            except (ValueError, TypeError):
                pass
        digits = re.sub(r'[^0-9]', '', price_str)
        if not digits:
            return "ì •ë³´ ì—†ìŒ"
        try:
            return f"{int(digits):,}ì›"
        except (ValueError, TypeError):
            return "ì •ë³´ ì—†ìŒ"

    @staticmethod
    def _clean_shipping_fee(shipping_input):
        shipping_str = str(shipping_input).strip()
        if not shipping_str or "ì •ë³´ ì—†ìŒ" in shipping_str: return "ì •ë³´ ì—†ìŒ"
        for part in re.split(r'[,()]', shipping_str):
            p = part.lower().strip()
            if 'í¬í•¨' in p: return "ë°°ì†¡ë¹„ í¬í•¨"
            if any(k in p for k in ['ë¬´ë£Œ', 'ë¬´ë°°', 'free']): return "ë¬´ë£Œ"
        if any(k in shipping_str for k in ["ë°°ì†¡ë¹„", "ì°©ë¶ˆ"]) and any(c.isdigit() for c in shipping_str): return shipping_str
        return "ì •ë³´ ì—†ìŒ"
    
    @staticmethod
    def _extract_shipping_from_title(title: str) -> str:
        match = re.search(r'\(([^)]+)\)$', title)
        if match:
            for part in match.group(1).split('/'):
                fee = BaseScraper._clean_shipping_fee(part)
                if fee != 'ì •ë³´ ì—†ìŒ': return fee
        if any(k in title for k in ['ë¬´ë°°', 'ë¬´ë£Œë°°ì†¡', 'íƒë°°ë¹„ í¬í•¨', 'ë¬´ë£Œ']): return "ë¬´ë£Œ"
        return "ì •ë³´ ì—†ìŒ"

    @staticmethod
    def _extract_price_from_title(title: str) -> str:
        match = re.search(r'\(([^)]+)\)$', title)
        if match:
            for part in match.group(1).split('/'):
                price = BaseScraper._clean_price(part)
                if price not in ["ê°€ê²© ìƒì´", "ì •ë³´ ì—†ìŒ"]: return price
        return "ì •ë³´ ì—†ìŒ"
    
    @staticmethod
    def _clean_shop_name(shop_name_input):
        shop_name_str = str(shop_name_input).strip()
        if not shop_name_str: return "ì •ë³´ ì—†ìŒ"
        lower = shop_name_str.lower()
        shop_map = {
            # í•´ì™¸ ì‡¼í•‘ëª°
            "ì•Œë¦¬ìµìŠ¤í”„ë ˆìŠ¤": "ì•Œë¦¬", "aliexpress": "ì•Œë¦¬", "ì•Œë¦¬": "ì•Œë¦¬",
            "alibaba": "ì•Œë¦¬ë°”ë°”", "ì•Œë¦¬ë°”ë°”": "ì•Œë¦¬ë°”ë°”",
            "amazon": "Amazon", "ì•„ë§ˆì¡´": "Amazon",
            "ebay": "eBay", "ì´ë² ì´": "eBay",
            "qoo10": "Qoo10", "íí…": "Qoo10",
            "wish": "Wish",

            # êµ­ë‚´ ëŒ€í˜• ì‡¼í•‘ëª°
            "gë§ˆì¼“": "Gë§ˆì¼“", "gmarket": "Gë§ˆì¼“",
            "ì˜¥ì…˜": "ì˜¥ì…˜", "auction": "ì˜¥ì…˜",
            "11ë²ˆê°€": "11ë²ˆê°€", "11st": "11ë²ˆê°€",
            "ì¿ íŒ¡": "ì¿ íŒ¡", "coupang": "ì¿ íŒ¡",
            "í‹°ëª¬": "í‹°ëª¬", "tmon": "í‹°ëª¬",
            "ìœ„ë©”í”„": "ìœ„ë©”í”„", "wemakeprice": "ìœ„ë©”í”„",
            "ì¸í„°íŒŒí¬": "ì¸í„°íŒŒí¬", "interpark": "ì¸í„°íŒŒí¬",
            
            # ë„¤ì´ë²„ ê´€ë ¨
            "ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´": "ë„¤ì´ë²„", "naver smartstore": "ë„¤ì´ë²„", "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´": "ë„¤ì´ë²„",
            "naver": "ë„¤ì´ë²„", "ë„¤ì´ë²„ì‡¼í•‘": "ë„¤ì´ë²„",

            # ìœ í†µ/ë¦¬í…Œì¼
            "ë¡¯ë°ì˜¨": "ë¡¯ë°ON", "lotteon": "ë¡¯ë°ON", "ë¡¯ë°ë§ˆíŠ¸": "ë¡¯ë°ON",
            "ssg": "SSG", "ì“±": "SSG", "ì‹ ì„¸ê³„": "SSG",
            "costco": "Costco", "ì½”ìŠ¤íŠ¸ì½”": "Costco",
            "í•˜ì´ë§ˆíŠ¸": "í•˜ì´ë§ˆíŠ¸", "hi-mart": "í•˜ì´ë§ˆíŠ¸",
            "í™ˆí”ŒëŸ¬ìŠ¤": "í™ˆí”ŒëŸ¬ìŠ¤", "homeplus": "í™ˆí”ŒëŸ¬ìŠ¤",
            "ì´ë§ˆíŠ¸": "ì´ë§ˆíŠ¸", "emart": "ì´ë§ˆíŠ¸",
            "ì˜¬ë¦¬ë¸Œì˜": "ì˜¬ë¦¬ë¸Œì˜", "oliveyoung": "ì˜¬ë¦¬ë¸Œì˜",
            
            # íŒ¨ì…˜/ë·°í‹° ì „ë¬¸ëª°
            "ë¬´ì‹ ì‚¬": "ë¬´ì‹ ì‚¬", "musinsa": "ë¬´ì‹ ì‚¬",
            "29cm": "29CM",
            "ë¸Œëœë””": "ë¸Œëœë””", "brandi": "ë¸Œëœë””",

            # ê²Œì„/ì½˜í…ì¸ 
            "psìŠ¤í† ì–´": "PlayStation Store", "í”Œë ˆì´ìŠ¤í…Œì´ì…˜": "PlayStation Store",
            
            # ê°€ê²©ë¹„êµ
            "ë‹¤ë‚˜ì™€": "ë‹¤ë‚˜ì™€", "danawa": "ë‹¤ë‚˜ì™€",

            # ìŒì‹/í”„ëœì°¨ì´ì¦ˆ
            "ìŠ¤íƒ€ë²…ìŠ¤": "ìŠ¤íƒ€ë²…ìŠ¤", "starbucks": "ìŠ¤íƒ€ë²…ìŠ¤",
            "ë§¥ë„ë‚ ë“œ": "ë§¥ë„ë‚ ë“œ", "mcdonald": "ë§¥ë„ë‚ ë“œ",
            "ë²„ê±°í‚¹": "ë²„ê±°í‚¹", "burgerking": "ë²„ê±°í‚¹",
            "ë¡¯ë°ë¦¬ì•„": "ë¡¯ë°ë¦¬ì•„",
            "kfc": "KFC",
            "í”¼ìí—›": "í”¼ìí—›",
            "ë„ë¯¸ë…¸í”¼ì": "ë„ë¯¸ë…¸í”¼ì",

            # ì¹´ë“œì‚¬
            "nhë†í˜‘ì¹´ë“œ": "NHë†í˜‘ì¹´ë“œ", "ë†í˜‘ì¹´ë“œ": "NHë†í˜‘ì¹´ë“œ",
            "ì‹ í•œì¹´ë“œ": "ì‹ í•œì¹´ë“œ", "shinhan": "ì‹ í•œì¹´ë“œ",
            "ì‚¼ì„±ì¹´ë“œ": "ì‚¼ì„±ì¹´ë“œ", "samsung": "ì‚¼ì„±ì¹´ë“œ",
            "í˜„ëŒ€ì¹´ë“œ": "í˜„ëŒ€ì¹´ë“œ", "hyundai": "í˜„ëŒ€ì¹´ë“œ",
            "kbêµ­ë¯¼ì¹´ë“œ": "KBêµ­ë¯¼ì¹´ë“œ", "êµ­ë¯¼ì¹´ë“œ": "KBêµ­ë¯¼ì¹´ë“œ",
            "í•˜ë‚˜ì¹´ë“œ": "í•˜ë‚˜ì¹´ë“œ", "hanacard": "í•˜ë‚˜ì¹´ë“œ",

            # í˜ì´ ì„œë¹„ìŠ¤
            "ì¹´ì¹´ì˜¤í˜ì´": "ì¹´ì¹´ì˜¤í˜ì´", "kakaopay": "ì¹´ì¹´ì˜¤í˜ì´",
            "ë„¤ì´ë²„í˜ì´": "ë„¤ì´ë²„í˜ì´", "naverpay": "ë„¤ì´ë²„í˜ì´",
            "í† ìŠ¤": "í† ìŠ¤", "toss": "í† ìŠ¤",
            "í˜ì´ì½”": "í˜ì´ì½”", "payco": "í˜ì´ì½”",
        }
        for alias, name in shop_map.items():
            if alias in lower: return name
        return shop_name_str

    @staticmethod
    def _infer_shop_name_from_link(ecommerce_link: str) -> str:
        """ì‡¼í•‘ëª° ë§í¬ì—ì„œ ì‡¼í•‘ëª° ì´ë¦„ì„ ì¶”ë¡ í•©ë‹ˆë‹¤."""
        if not ecommerce_link:
            return "ì •ë³´ ì—†ìŒ"
            
        try:
            domain = urlparse(ecommerce_link).netloc.lower()
            logger.info(f" - Inferring shop name from domain: {domain}")
            
            # ë„ë©”ì¸ë³„ ì‡¼í•‘ëª° ë§¤í•‘
            domain_map = {
                # í•´ì™¸ ì‡¼í•‘ëª°
                "aliexpress.com": "ì•Œë¦¬",
                "alibaba.com": "ì•Œë¦¬ë°”ë°”", 
                "amazon.com": "Amazon",
                "amazon.co.kr": "Amazon",
                "ebay.com": "eBay",
                "wish.com": "Wish",
                
                # êµ­ë‚´ ëŒ€í˜• ì‡¼í•‘ëª°
                "coupang.com": "ì¿ íŒ¡",
                "gmarket.co.kr": "Gë§ˆì¼“",
                "auction.co.kr": "ì˜¥ì…˜",
                "11st.co.kr": "11ë²ˆê°€",
                "lotteon.com": "ë¡¯ë°ON",
                "tmon.co.kr": "í‹°ëª¬",
                "wemakeprice.com": "ìœ„ë©”í”„", # .co.kr -> .com ë³€ê²½ ê°€ëŠ¥ì„±
                "ssg.com": "SSG",
                "qoo10.com": "Qoo10", # .co.kr -> .com ë³€ê²½ ê°€ëŠ¥ì„±
                
                # ë„¤ì´ë²„ ê´€ë ¨
                "smartstore.naver.com": "ë„¤ì´ë²„",
                "shopping.naver.com": "ë„¤ì´ë²„",
                "m.naver.com": "ë„¤ì´ë²„",
                
                # ë¸Œëœë“œ/ì„œë¹„ìŠ¤
                "musinsa.com": "ë¬´ì‹ ì‚¬",
                "oliveyoung.co.kr": "ì˜¬ë¦¬ë¸Œì˜",
                "29cm.co.kr": "29CM",  
                "brandi.co.kr": "ë¸Œëœë””",
                "interpark.com": "ì¸í„°íŒŒí¬",
                "e-himart.co.kr": "í•˜ì´ë§ˆíŠ¸", # í•˜ì´ë§ˆíŠ¸ ë„ë©”ì¸ ë³€ê²½ ê°€ëŠ¥ì„±
                "danawa.com": "ë‹¤ë‚˜ì™€",
                "homeplus.co.kr": "í™ˆí”ŒëŸ¬ìŠ¤",
                "emart.ssg.com": "ì´ë§ˆíŠ¸", # ì´ë§ˆíŠ¸ ë„ë©”ì¸
                
                # ì¹´ë“œ/í˜ì´ ì„œë¹„ìŠ¤
                "kakaopay.com": "ì¹´ì¹´ì˜¤í˜ì´",
                "pay.naver.com": "ë„¤ì´ë²„í˜ì´",
                "toss.im": "í† ìŠ¤",
                "payco.com": "í˜ì´ì½”",
                
                # ìŒì‹ì /í”„ëœì°¨ì´ì¦ˆ
                "starbucks.co.kr": "ìŠ¤íƒ€ë²…ìŠ¤",
                "mcdonalds.co.kr": "ë§¥ë„ë‚ ë“œ", 
                "burgerking.co.kr": "ë²„ê±°í‚¹",
            }
            
            # ì •í™•í•œ ë„ë©”ì¸ ë§¤ì¹­
            if domain in domain_map:
                shop_name = domain_map[domain]
                logger.info(f" - Matched exact domain '{domain}' -> '{shop_name}'")
                return shop_name
            
            # ì„œë¸Œë„ë©”ì¸ í¬í•¨ ë§¤ì¹­
            for domain_key, shop_name in domain_map.items():
                if domain.endswith(domain_key):
                    logger.info(f" - Matched subdomain '{domain_key}' -> '{shop_name}'")
                    return shop_name
            
            logger.info(f" - No matching shop found for domain: {domain}")
            return "ì •ë³´ ì—†ìŒ"
            
        except Exception as e:
            logger.warning(f" - Failed to infer shop name from link: {e}")
            return "ì •ë³´ ì—†ìŒ"

    @staticmethod
    def _infer_shop_name_from_content(full_title: str, content_html: str) -> str:
        """ë³¸ë¬¸ì´ë‚˜ íƒ€ì´í‹€ì—ì„œ ì‹¤ì œ ìƒí’ˆì„ íŒŒëŠ” ê³³ì„ ì°¾ì•„ ì‡¼í•‘ëª° ì´ë¦„ì„ ì¶”ë¡ í•©ë‹ˆë‹¤."""
        logger.info(" - Inferring shop name from title/content...")
        
        # íƒ€ì´í‹€ê³¼ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê²°í•©
        content_text = ""
        if content_html:
            try:
                soup = BeautifulSoup(content_html, 'html.parser')
                content_text = soup.get_text().lower()
            except:
                content_text = ""
        
        combined_text = (full_title + " " + content_text).lower()
        
        # 1. ì¿ í°/ì ë¦½ ê´€ë ¨ íŒ¨í„´ í™•ì¸ (ë” ë†’ì€ ìš°ì„ ìˆœìœ„)
        coupon_patterns = [
            # ì¹´ë“œì‚¬
            (r'(nhë†í˜‘ì¹´ë“œ|ë†í˜‘ì¹´ë“œ)', "NHë†í˜‘ì¹´ë“œ"),
            (r'(ì‹ í•œì¹´ë“œ)', "ì‹ í•œì¹´ë“œ"),
            (r'(ì‚¼ì„±ì¹´ë“œ)', "ì‚¼ì„±ì¹´ë“œ"),
            (r'(í˜„ëŒ€ì¹´ë“œ)', "í˜„ëŒ€ì¹´ë“œ"),
            (r'(kbêµ­ë¯¼ì¹´ë“œ|êµ­ë¯¼ì¹´ë“œ)', "KBêµ­ë¯¼ì¹´ë“œ"),
            (r'(í•˜ë‚˜ì¹´ë“œ)', "í•˜ë‚˜ì¹´ë“œ"),
            
            # í˜ì´ ì„œë¹„ìŠ¤
            (r'(ì¹´ì¹´ì˜¤í˜ì´)', "ì¹´ì¹´ì˜¤í˜ì´"),
            (r'(ë„¤ì´ë²„í˜ì´)', "ë„¤ì´ë²„í˜ì´"),
            (r'(í† ìŠ¤)', "í† ìŠ¤"),
            (r'(í˜ì´ì½”)', "í˜ì´ì½”"),
            
            # í”„ëœì°¨ì´ì¦ˆ
            (r'(ìŠ¤íƒ€ë²…ìŠ¤)', "ìŠ¤íƒ€ë²…ìŠ¤"),
            (r'(ë§¥ë„ë‚ ë“œ)', "ë§¥ë„ë‚ ë“œ"),
            (r'(ë²„ê±°í‚¹)', "ë²„ê±°í‚¹"),
            (r'(ë¡¯ë°ë¦¬ì•„)', "ë¡¯ë°ë¦¬ì•„"),
            (r'(kfc)', "KFC"),
            (r'(í”¼ìí—›)', "í”¼ìí—›"),
            (r'(ë„ë¯¸ë…¸í”¼ì)', "ë„ë¯¸ë…¸í”¼ì"),
        ]
        
        for pattern, shop_name in coupon_patterns:
            if re.search(pattern, combined_text):
                logger.info(f" - Found coupon/reward pattern: '{shop_name}'")
                return shop_name
        
        # 2. ì¼ë°˜ ì‡¼í•‘ëª° íŒ¨í„´ í™•ì¸
        shop_patterns = [
            (r'ì•Œë¦¬ìµìŠ¤í”„ë ˆìŠ¤|aliexpress', "ì•Œë¦¬"),
            (r'ì¿ íŒ¡|coupang', "ì¿ íŒ¡"),
            (r'ì§€ë§ˆì¼“|gë§ˆì¼“|gmarket', "Gë§ˆì¼“"),
            (r'ì˜¥ì…˜|auction', "ì˜¥ì…˜"),
            (r'11ë²ˆê°€|11st', "11ë²ˆê°€"),
            (r'ë„¤ì´ë²„.*?ìŠ¤í† ì–´|ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´', "ë„¤ì´ë²„"),
            (r'ë¡¯ë°ì˜¨|lotteon', "ë¡¯ë°ON"),
            (r'í‹°ëª¬|tmon', "í‹°ëª¬"),
            (r'ìœ„ë©”í”„|wemakeprice', "ìœ„ë©”í”„"),
            (r'ssg|ì“±', "SSG"),
            (r'íí…|qoo10', "Qoo10"),
            (r'ì•„ë§ˆì¡´|amazon', "Amazon"),
            (r'ì´ë² ì´|ebay', "eBay"),
        ]
        
        for pattern, shop_name in shop_patterns:
            if re.search(pattern, combined_text):
                logger.info(f" - Found shop pattern: '{shop_name}' from content")
                return shop_name
        
        # 3. íƒ€ì´í‹€ì—ì„œ ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ
        bracket_match = re.search(r'\[([^\]]+)\]', full_title)
        if bracket_match:
            bracket_content = bracket_match.group(1).strip()
            # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ë‹¤ì‹œ clean_shop_nameìœ¼ë¡œ í™•ì¸
            cleaned_name = BaseScraper._clean_shop_name(bracket_content)
            if cleaned_name != 'ì •ë³´ ì—†ìŒ' and cleaned_name != bracket_content:
                logger.info(f" - Using bracket content as shop name: '{cleaned_name}'")
                return cleaned_name
        
        logger.info(" - Could not infer shop name from content")
        return "ì •ë³´ ì—†ìŒ"
                
    def _process_detail_pages(self, temp_deals_info: list) -> list:
        deals_data = []
        for deal_info in temp_deals_info:
            post_link = deal_info.get('post_link', '')
            try:
                logger.info(f"Processing post_link: {post_link}")
                full_title_raw = deal_info['full_title']
                full_title = re.sub(r'\s*\(\d+\)$', '', full_title_raw).strip()
                logger.info(f"Processing deal: {full_title[:50]}...")
                self.driver.get(post_link)
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                detail_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                possible_selectors = ['.view_content article', '#bo_v_atc', '.bo_v_con', 'div.view-content', 'td.board-contents', '.view_content']
                
                content_element = None
                for selector in possible_selectors:
                    content_element = detail_soup.select_one(selector)
                    if content_element:
                        logger.info(f"ì½˜í…ì¸  ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤ (selector: '{selector}')")
                        break
                
                if not content_element:
                    logger.warning(f"ì§€ì •ëœ ì½˜í…ì¸  ì„ íƒìë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. body ì „ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {post_link}")
                    content_element = detail_soup.select_one('body')

                if not content_element:
                    logger.error(f"Body íƒœê·¸ì¡°ì°¨ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ê²Œì‹œë¬¼ì„ ê±´ë„ˆëœë‹ˆë‹¤: {post_link}")
                    continue
                    
                content_html = str(content_element)

                category_from_list = deal_info.get('list_category')
                shop_name_from_list = deal_info.get('list_shop_name', 'ì •ë³´ ì—†ìŒ')
                price_from_list = deal_info.get('list_price') or self._extract_price_from_title(full_title)
                shipping_fee_from_list = deal_info.get('list_shipping_fee') or self._extract_shipping_from_title(full_title)
                
                category_ai_result = ai_parser.parse_title_with_ai(full_title) or {}
                category = self._clean_text(category_from_list or category_ai_result.get('category', 'ê¸°íƒ€'))
                
                ai_result = ai_parser.parse_content_with_ai(content_html=content_html, post_link=post_link, original_title=full_title)
                if not ai_result or not ai_result.get('deals'):
                    logger.warning(f"AI parsing failed for link: {post_link}")
                    continue
                
                logger.info(f"  [AI Multi-Deal Analysis] Found {len(ai_result['deals'])} deals.")
                
                # --- âœ¨ [ìˆ˜ì •] ì´ë¯¸ì§€ ëŒ€í‘œ ì´ë¯¸ì§€ ê²°ì • ë¡œì§ ---
                list_image_url = deal_info.get('original_image_url')
                valid_images_from_content = [urljoin(self.base_url, img.get('src') or '') for img in content_element.select('img') if img.get('src')]
                
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€(placeholder, icon ë“±) í•„í„°ë§
                valid_images_from_content = [img for img in valid_images_from_content if not re.search(r'icon|emoticon|expand', img)]

                # ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒ
                post_representative_image = None
                if list_image_url:
                    post_representative_image = list_image_url
                elif valid_images_from_content:
                    post_representative_image = valid_images_from_content[0]

                logger.info(f"  [Image Debug] Post Representative Image: {post_representative_image}")
                # --- ìˆ˜ì • ì™„ë£Œ ---

                group_id = hashlib.md5(post_link.encode()).hexdigest()

                for idx, deal_item in enumerate(ai_result['deals']):
                    product_title = self._clean_text(deal_item.get('product_title'))
                    logger.info(f"    [Deal {idx+1}] AI Product: {product_title}")
                    
                    final_price = self._get_final_price(price_from_list=price_from_list, price_from_ai=deal_item.get('price', 'ì •ë³´ ì—†ìŒ'))
                    final_shipping_fee = self._get_final_shipping_fee(shipping_fee_from_list=shipping_fee_from_list, shipping_fee_from_ai=deal_item.get('shipping_fee', 'ì •ë³´ ì—†ìŒ'))
                    
                    shop_name = shop_name_from_list
                    if shop_name == 'ì •ë³´ ì—†ìŒ': shop_name = self._clean_shop_name(ai_result.get('shop_name'))

                    raw_ecommerce_link = deal_item.get('ecommerce_link', '')
                    final_ecommerce_link = self._resolve_redirect(raw_ecommerce_link)
                    
                    if final_ecommerce_link:
                        inferred_shop_from_link = self._infer_shop_name_from_link(final_ecommerce_link)
                        if inferred_shop_from_link != 'ì •ë³´ ì—†ìŒ':
                            shop_name = inferred_shop_from_link
                    
                    if shop_name == 'ì •ë³´ ì—†ìŒ':
                        inferred_shop_from_content = self._infer_shop_name_from_content(full_title, content_html)
                        if inferred_shop_from_content != 'ì •ë³´ ì—†ìŒ':
                            shop_name = inferred_shop_from_content

                    # ëª¨ë“  ë”œì— ë™ì¼í•œ ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ í• ë‹¹
                    web_path, _, _ = self._download_and_get_local_path(post_representative_image, -1, perform_ocr=False)
                    
                    is_options_deal = deal_item.get('deal_type') == 'Type A: Options Deal'
                    options_data_json = json.dumps(deal_item.get('options', [])) if is_options_deal else None
                    base_product_name = deal_item.get('base_product_name') if is_options_deal else product_title
                    
                    new_deal = models.Deal(
                        source_community_id=self.community_entry.id, title=product_title, post_link=post_link,
                        ecommerce_link=final_ecommerce_link, shop_name=shop_name, price=final_price,
                        shipping_fee=final_shipping_fee, category=category, is_closed=deal_item.get('is_closed', False),
                        deal_type=deal_item.get('deal_type', 'ì¼ë°˜'), image_url=web_path, content_html=content_html,
                        group_id=group_id, has_options=is_options_deal, options_data=options_data_json,
                        base_product_name=base_product_name
                    )
                    deals_data.append({'deal': new_deal, 'original_image_url': post_representative_image})
            except Exception as e:
                logger.error(f"Error processing {post_link}: {e}", exc_info=True)
        return deals_data

    def _clean_text(self, text_input):
        if not text_input or not str(text_input).strip(): return "ì •ë³´ ì—†ìŒ"
        return str(text_input).strip()

    def _ocr_image(self, image_path: str) -> str:
        if not image_path or not os.path.exists(image_path): return ""
        try:
            result = BaseScraper.ocr_reader.readtext(image_path)
            image_text = ' '.join([text for _, text, _ in result])
            if image_text.strip(): logger.debug(f"[OCR] Extracted: '{image_text.strip()[:100]}...'")
            return image_text
        except Exception as e:
            logger.warning(f"[OCR] Failed for image {image_path}: {e}")
            return ""

    def _resolve_redirect(self, url: str) -> str:
        if not url or not isinstance(url, str):
            logger.info("   - No ecommerce_link provided by AI.")
            return ''
        original_url = url

        # --- âœ¨ [ìˆ˜ì •] ë¹„ì •ìƒì ì¸ URLì„ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ ë³´ê°• ---
        if url.startswith('http:s'):
            url = url.replace('http:s', 'https', 1)
        elif url.startswith('https.'):
            url = url.replace('https.', 'https://', 1)
        elif url.startswith('http.'):
            url = url.replace('http.', 'http://', 1)
        # 'http:'ë¡œ ì‹œì‘í•˜ì§€ë§Œ 'http://'ê°€ ì•„ë‹Œ ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ ì¶”ê°€
        elif url.startswith('http:') and not url.startswith('http://'):
            url = url.replace('http:', 'http://', 1)
        elif not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        # --- ìˆ˜ì • ì™„ë£Œ ---

        logger.info(f"   - [Redirect] 1. AI raw link: {original_url}")
        if original_url != url:
            logger.info(f"   - [Redirect] 1.1. Fixed URL: {url}")

        try:
            decoded_url = BaseScraper._parse_ecommerce_link(url)
            logger.info(f"   - [Redirect] 2. Decoded link: {decoded_url}")

            if not decoded_url.startswith(('http://', 'https://')):
                logger.warning(f"   - [Redirect] 3. Invalid URL after decoding. Returning as-is: {decoded_url}")
                return decoded_url

            self.driver.get(decoded_url)
            try:
                WebDriverWait(self.driver, 2).until(
                    lambda driver: driver.execute_script("return document.readyState") == "complete"
                )
            except Exception:
                logger.warning(f"   - [Redirect] Page load timeout for {decoded_url}. Proceeding anyway.")
            
            final_url = self.driver.current_url
            logger.info(f"   - [Redirect] 3. Final resolved URL: {final_url}")
            return final_url

        except Exception as e:
            logger.error(f"   - [Redirect] 3. Failed to resolve redirect for {url}: {e}", exc_info=True)
            try:
                current_url_on_fail = self.driver.current_url
                logger.warning(f"   - [Redirect] Error occurred. Returning current URL: {current_url_on_fail}")
                return current_url_on_fail
            except:
                return decoded_url if 'decoded_url' in locals() else url

    def _get_final_price(self, price_from_list: str, price_from_ai: str) -> str:
        # --- âœ¨ [ìˆ˜ì •] ê°€ê²© ê²°ì • ë¡œì§ ë³€ê²½ ---
        list_price = self._clean_price(price_from_list)
        # ëª©ë¡(ì œëª©)ì—ì„œ ê°€ì ¸ì˜¨ ê°€ê²©ì´ ìœ íš¨í•˜ë©´ AI ê²°ê³¼ì™€ ìƒê´€ì—†ì´ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©
        if list_price not in ["ì •ë³´ ì—†ìŒ", "ê°€ê²© ìƒì´"]:
            return list_price
        
        # ëª©ë¡ ì •ë³´ê°€ ì—†ì„ ë•Œë§Œ AI ê°€ê²© ì‚¬ìš©
        ai_price = self._clean_price(price_from_ai)
        if ai_price not in ["ì •ë³´ ì—†ìŒ", "ê°€ê²© ìƒì´"] and not any(kw in price_from_ai for kw in ['í• ì¸', 'ì¿ í°', '~', 'N/A', 'ì ë¦½']):
            return ai_price
            
        return ai_price
        # --- ìˆ˜ì • ì™„ë£Œ ---

    def _get_final_shipping_fee(self, shipping_fee_from_list: str, shipping_fee_from_ai: str) -> str:
        if shipping_fee_from_list != 'ì •ë³´ ì—†ìŒ':
            return shipping_fee_from_list
        cleaned_ai_fee = self._clean_shipping_fee(shipping_fee_from_ai)
        if cleaned_ai_fee != 'ì •ë³´ ì—†ìŒ':
            return cleaned_ai_fee
        return "ì •ë³´ ì—†ìŒ"