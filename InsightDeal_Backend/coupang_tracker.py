import requests
from bs4 import BeautifulSoup
import re
import logging
from urllib.parse import urlparse, parse_qs
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth
from datetime import datetime
import time
import random
from typing import Optional, Dict

logger = logging.getLogger(__name__)

class CoupangTracker:
    """ì¿ íŒ¡ ìƒí’ˆ ê°€ê²© ì¶”ì  ì‹œìŠ¤í…œ (2025ë…„ ìµœì‹  ë²„ì „)"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        }
    
    def extract_product_info(self, product_url: str) -> Optional[Dict]:
        """ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ (ì°¨ë‹¨ ìš°íšŒ í¬í•¨)"""
        try:
            logger.info(f"ğŸ› [Coupang] Extracting info from: {product_url[:60]}...")
            
            # Chrome ì˜µì…˜ ì„¤ì • (ì¿ íŒ¡ ì°¨ë‹¨ ìš°íšŒ)
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-logging')
            chrome_options.add_argument('--log-level=3')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument(f'--user-agent={self.headers["User-Agent"]}')
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                # Stealth ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
                stealth(driver,
                    languages=["ko-KR", "ko", "en-US", "en"],
                    vendor="Google Inc.",
                    platform="Win32",
                    webgl_vendor="Intel Inc.",
                    renderer="Intel Iris OpenGL Engine",
                    fix_hairline=True
                )
                
                driver.set_page_load_timeout(15)
                driver.implicitly_wait(10)
                
                # í˜ì´ì§€ ë¡œë“œ
                driver.get(product_url)
                
                # ë™ì  ë¡œë”© ëŒ€ê¸°
                wait = WebDriverWait(driver, 15)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                
                # ì¶”ê°€ ëŒ€ê¸° (ì¿ íŒ¡ ë¡œë”© ì™„ë£Œ)
                time.sleep(random.uniform(2, 4))
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # 1. ìƒí’ˆëª… ì¶”ì¶œ (2025ë…„ ìµœì‹  ì…€ë ‰í„°)
                title_selectors = [
                    'h1.prod-buy-header__title',
                    '.prod-buy-header__title',
                    'h1[data-testid="product-title"]',
                    '.sdp-product__title',
                    'h1.prod-buy__title',
                    '.product-title h1'
                ]
                
                title = None
                for selector in title_selectors:
                    title_element = soup.select_one(selector)
                    if title_element:
                        title = title_element.get_text().strip()
                        logger.debug(f"âœ… [Coupang] Title found with selector: {selector}")
                        break
                
                # 2. ê°€ê²© ì¶”ì¶œ (2025ë…„ ìµœì‹  ì…€ë ‰í„°)
                price_selectors = [
                    '.total-price strong',
                    '.price-value strong',
                    '[data-testid="price-value"] strong',
                    '.prod-price .total-price',
                    '.prod-sale-price .total-price',
                    '.price .total-price',
                    '.price em',
                    'strong.price'
                ]
                
                price = None
                original_price = None
                
                for selector in price_selectors:
                    price_element = soup.select_one(selector)
                    if price_element:
                        price_text = price_element.get_text().strip()
                        # ìˆ«ìë§Œ ì¶”ì¶œ (,ì› ì œê±°)
                        price_digits = re.sub(r'[^0-9]', '', price_text)
                        if price_digits and len(price_digits) >= 3:
                            price = int(price_digits)
                            logger.debug(f"âœ… [Coupang] Price found: {price:,}ì› (selector: {selector})")
                            break
                
                # 3. í• ì¸ ì „ ê°€ê²© ì¶”ì¶œ
                original_price_selectors = [
                    '.prod-origin-price',
                    '.price-original',
                    '.original-price',
                    'del.price',
                    '.discount-price'
                ]
                
                for selector in original_price_selectors:
                    orig_element = soup.select_one(selector)
                    if orig_element:
                        orig_text = orig_element.get_text().strip()
                        orig_digits = re.sub(r'[^0-9]', '', orig_text)
                        if orig_digits and len(orig_digits) >= 3:
                            original_price = int(orig_digits)
                            break
                
                # 4. ìƒí’ˆ ì´ë¯¸ì§€ ì¶”ì¶œ
                image_url = None
                image_selectors = [
                    '.prod-image__detail img',
                    '.prod-image img',
                    '.product-image img',
                    'img.prod-image__main'
                ]
                
                for selector in image_selectors:
                    img_element = soup.select_one(selector)
                    if img_element:
                        img_src = img_element.get('src') or img_element.get('data-src')
                        if img_src:
                            # URL ì •ê·œí™”
                            if img_src.startswith('//'):
                                image_url = 'https:' + img_src
                            elif img_src.startswith('/'):
                                image_url = 'https://image.coupang.com' + img_src
                            else:
                                image_url = img_src
                            break
                
                # 5. ë¸Œëœë“œ/íŒë§¤ì ì •ë³´
                brand = None
                brand_selectors = [
                    '.prod-brand-name',
                    '.vendor-name',
                    '.brand-name'
                ]
                
                for selector in brand_selectors:
                    brand_element = soup.select_one(selector)
                    if brand_element:
                        brand = brand_element.get_text().strip()
                        break
                
                # 6. ìƒí’ˆ ID ì¶”ì¶œ (URLì—ì„œ)
                product_id = self._extract_product_id(product_url)
                
                # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                result = {
                    'product_id': product_id,
                    'title': title or 'ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ',
                    'price': price or 0,
                    'original_price': original_price,
                    'image_url': image_url,
                    'brand': brand,
                    'url': product_url,
                    'extracted_at': datetime.now().isoformat(),
                    'is_available': price is not None and price > 0
                }
                
                # í• ì¸ìœ¨ ê³„ì‚°
                if price and original_price and original_price > price:
                    discount_rate = int(((original_price - price) / original_price) * 100)
                    result['discount_rate'] = discount_rate
                
                success_msg = f"âœ… [Coupang] Success: {title[:40] if title else 'No title'}"
                if price:
                    success_msg += f" - {price:,}ì›"
                    if original_price:
                        success_msg += f" (í• ì¸ì „: {original_price:,}ì›)"
                
                logger.info(success_msg)
                return result
                
            finally:
                driver.quit()
                
        except Exception as e:
            logger.error(f"âŒ [Coupang] Extraction failed for {product_url}: {e}")
            return None
    
    def _extract_product_id(self, url: str) -> Optional[str]:
        """ì¿ íŒ¡ ìƒí’ˆ ID ì¶”ì¶œ"""
        try:
            # URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì§€ì›)
            patterns = [
                r'/products/(\d+)',  # /products/123456
                r'itemId=(\d+)',     # ?itemId=123456  
                r'vendorItemId=(\d+)' # ?vendorItemId=123456
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            
            for param in ['itemId', 'vendorItemId', 'productId']:
                if param in query_params:
                    return query_params[param][0]
            
            return None
            
        except Exception:
            return None
    
    def is_valid_coupang_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ì¿ íŒ¡ URL ê²€ì¦"""
        try:
            parsed = urlparse(url)
            domain_valid = 'coupang.com' in parsed.netloc.lower()
            path_valid = any(keyword in parsed.path for keyword in ['products', 'vp/products'])
            
            return domain_valid and (path_valid or 'itemId' in parsed.query)
        except:
            return False
    
    def get_clean_coupang_url(self, url: str) -> str:
        """ì¿ íŒ¡ URL ì •ë¦¬ (ì¶”ì ìš©)"""
        try:
            parsed = urlparse(url)
            product_id = self._extract_product_id(url)
            
            if product_id:
                # ê¹”ë”í•œ ì¿ íŒ¡ URL ìƒì„±
                return f"https://www.coupang.com/vp/products/{product_id}"
            else:
                return url
        except:
            return url
    
    def check_price_change(self, current_data: Dict, previous_data: Dict) -> Dict:
        """ê°€ê²© ë³€ë™ ë¶„ì„"""
        current_price = current_data.get('price', 0)
        previous_price = previous_data.get('price', 0)
        
        if not current_price or not previous_price:
            return {'changed': False}
        
        price_diff = current_price - previous_price
        change_rate = (price_diff / previous_price) * 100
        
        result = {
            'changed': price_diff != 0,
            'price_diff': price_diff,
            'change_rate': round(change_rate, 2),
            'direction': 'up' if price_diff > 0 else 'down' if price_diff < 0 else 'same'
        }
        
        if result['changed']:
            logger.info(f"ğŸ“ˆ [Coupang] Price changed: {previous_price:,}ì› â†’ {current_price:,}ì› ({change_rate:+.1f}%)")
        
        return result
    
    def batch_check_products(self, product_urls: list) -> Dict:
        """ì—¬ëŸ¬ ìƒí’ˆ ì¼ê´„ ê°€ê²© ì²´í¬"""
        results = {
            'success': [],
            'failed': [],
            'total_checked': len(product_urls),
            'checked_at': datetime.now().isoformat()
        }
        
        for i, url in enumerate(product_urls):
            try:
                logger.info(f"ğŸ”„ [Coupang Batch] Checking {i+1}/{len(product_urls)}: {url[:40]}...")
                
                product_info = self.extract_product_info(url)
                if product_info:
                    results['success'].append(product_info)
                else:
                    results['failed'].append({'url': url, 'error': 'Failed to extract'})
                
                # ìš”ì²­ ê°„ê²© (ì¿ íŒ¡ ì„œë²„ ë¶€í•˜ ë°©ì§€)
                if i < len(product_urls) - 1:
                    delay = random.uniform(3, 7)
                    logger.debug(f"â±ï¸ [Coupang Batch] Waiting {delay:.1f}s before next request...")
                    time.sleep(delay)
                    
            except Exception as e:
                logger.error(f"âŒ [Coupang Batch] Failed for {url}: {e}")
                results['failed'].append({'url': url, 'error': str(e)})
        
        logger.info(f"âœ… [Coupang Batch] Completed: {len(results['success'])}/{results['total_checked']} successful")
        return results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
coupang_tracker = CoupangTracker()